{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12f5311-9ea5-451e-a01c-3c22ddc8bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from torcheval import metrics\n",
    "\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a44dcf1-3798-4c13-b0a9-f16735166b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = np.array(['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection',\n",
    "                     'Disquietment', 'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear',\n",
    "                     'Happiness', 'Pain', 'Peace', 'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning'])\n",
    "\n",
    "emotion_encode = {e: i for i, e in enumerate(emotions)}\n",
    "emotion_decode = {i: e for e, i in emotion_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c81ac5-5a04-4a5b-9961-aa05471dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoticDataset(Dataset):\n",
    "    def __init__(self, data_type, subject_size, context_size, anns_dir, img_dir):\n",
    "        anns = scio.loadmat(anns_dir)[data_type]\n",
    "        self.anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)\n",
    "        self.img_dir = img_dir\n",
    "        self.subject_transform = transforms.Resize(subject_size)\n",
    "        self.context_transform = transforms.Resize(context_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.anns.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.anns[idx]\n",
    "\n",
    "        img_loc = \"../data/cvpr_emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item()\n",
    "        context_img = read_image(img_loc)\n",
    "        \n",
    "        bbox =  ann[\"person\"][\"body_bbox\"][0][0][0].astype(int)\n",
    "        subject_img = crop(context_img, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "        label = np.zeros(len(emotions), dtype=np.float32)\n",
    "        ems = [i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0][0][0][0][0]]\n",
    "        for e in ems:\n",
    "            label[emotion_encode[e]] = 1.\n",
    "\n",
    "        subject_img = self.subject_transform(subject_img.float())\n",
    "        context_img = self.context_transform(context_img.float())\n",
    "\n",
    "        if subject_img.shape[0] == 1:\n",
    "            subject_img = subject_img.repeat(3, 1, 1)\n",
    "            \n",
    "        if context_img.shape[0] == 1:\n",
    "            context_img = context_img.repeat(3, 1, 1)\n",
    "\n",
    "        return subject_img, context_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be692383-5210-4ae2-b34c-26c2a2ad1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_branch():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 96, (11, 1), stride=(4, 1)),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(96),\n",
    "        nn.Conv2d(96, 96, (1, 11), stride=(1, 4)),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(96),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        \n",
    "        nn.Conv2d(96, 256, (1, 5), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(256, 256, (5, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        \n",
    "        nn.Conv2d(256, 384, (1, 3), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        nn.Conv2d(384, 384, (3, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        \n",
    "        nn.Conv2d(384, 384, (1, 3), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        nn.Conv2d(384, 384, (3, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        \n",
    "        nn.Conv2d(384, 256, (1, 3), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(256, 256, (3, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002904bb-cc8f-4ea4-8c81-7959c3a0ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.subject = net_branch()\n",
    "        self.context = net_branch()\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(12800, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, len(emotions)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, s, c):\n",
    "        s = self.subject(s)\n",
    "        s = torch.flatten(s, start_dim=1)\n",
    "        \n",
    "        c = self.context(c)\n",
    "        c = torch.flatten(c, start_dim=1)\n",
    "\n",
    "        x = torch.cat((s, c), dim=1)\n",
    "        x = self.fusion(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaef8c15-7fd1-4b37-b710-a22a90b6a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4af59d47-7c33-4ef2-8ef7-05765e414d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_size = (224, 224)\n",
    "context_size = (224, 224)\n",
    "\n",
    "train_batch_size = 52\n",
    "val_batch_size = 52\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5088280b-72bd-41d0-b80b-f3a6f1fa77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmoticDataset(\"train\", subject_size, context_size, \"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\")\n",
    "train_dataloader = DataLoader(train_data, batch_size=train_batch_size)\n",
    "\n",
    "val_data = EmoticDataset(\"val\", subject_size, context_size, \"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\")\n",
    "val_dataloader = DataLoader(val_data, batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b69484bb-4517-4aa8-97fd-d17061318a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotic_net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9df2636-dfbe-4b65-9faa-18f0967f46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_dataloader.dataset)\n",
    "val_size = len(val_dataloader.dataset)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(emotic_net.parameters(), lr=.01)\n",
    "metric = metrics.MultilabelAUPRC(num_labels=len(emotions), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b34afa7-edb4-4ec7-a532-1cfd0a7020b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "loss: 0.181189  [12915/12915]\n",
      "calculating metric for:  [1705/1705]\n",
      "metric: 11.236251831054688 %\n",
      "\n",
      "Epoch 2/5\n",
      "loss: 0.176353  [12915/12915]\n",
      "calculating metric for:  [1705/1705]\n",
      "metric: 12.07455062866211 %\n",
      "\n",
      "Epoch 3/5\n",
      "loss: 0.170990  [12915/12915]\n",
      "calculating metric for:  [1705/1705]\n",
      "metric: 12.602082252502441 %\n",
      "\n",
      "Epoch 4/5\n",
      "loss: 0.165204  [12915/12915]\n",
      "calculating metric for:  [1705/1705]\n",
      "metric: 12.904195785522461 %\n",
      "\n",
      "Epoch 5/5\n",
      "loss: 0.202257  [ 2080/12915]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (S, C, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      6\u001b[0m     S, C, y \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mto(device), C\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43memotic_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, s, c)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, c):\n\u001b[0;32m---> 18\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubject\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(s, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext(c)\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/torch/lib/python3.10/site-packages/torch/nn/functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    emotic_net.train()\n",
    "    for batch, (S, C, y) in enumerate(train_dataloader):\n",
    "        S, C, y = S.to(device), C.to(device), y.to(device)\n",
    "        pred = emotic_net(S, C)\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss, current = loss.item(), min((batch + 1) * train_batch_size, train_size)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\", end=\"\\r\")\n",
    "        \n",
    "    print()\n",
    "\n",
    "    torch.save(emotic_net.state_dict(), \"../models/basic.pth\")\n",
    "    \n",
    "    metric.reset()\n",
    "    emotic_net.eval()\n",
    "    for batch, (S, C, y) in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            S, C, y = S.to(device), C.to(device), y.to(device)\n",
    "            pred = emotic_net(S, C)\n",
    "            metric.update(pred, y)\n",
    "    \n",
    "            current = min((batch + 1) * val_batch_size, val_size)\n",
    "            print(f\"calculating metric for:  [{current:>4d}/{val_size:>4d}]\", end=\"\\r\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"metric: {metric.compute() * 100} %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a19d840c-348a-4ec8-b011-d66e480e5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03e49069-97a6-4c73-aa64-0437e2a4f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 18           |        cudaMalloc retries: 18        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  12569 MiB |  12570 MiB |  32137 MiB |  19567 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  12569 MiB |  12570 MiB |  32137 MiB |  19567 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  12486 MiB |  12487 MiB |  31884 MiB |  19398 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  12640 MiB |  12640 MiB |  12640 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  72154 KiB |  93476 KiB |  39841 MiB |  39771 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |   20183    |   20183    |   36348    |   16165    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |   20183    |   20183    |   36348    |   16165    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |    2786    |    2786    |    2786    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |    1988    |    1990    |   12071    |   10083    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc5275e1-80af-4687-9e1e-8dcda8d050e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\r"
     ]
    }
   ],
   "source": [
    "for batch, (S, C, y) in enumerate(val_dataloader):\n",
    "    S, C, y = S.to(device), C.to(device), y.to(device)\n",
    "    print(f\"{batch}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20b011ff-2e56-4da8-ac1d-6f71b806fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32373e38-3afe-42ee-a2a0-d2aecd7ac194",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = emotic_net(data[0].to(device), data[1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "49ab6f67-1b1b-474b-b4ca-60520b8e33b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000, -0.0000, 0.3312, 1.0000, 0.1121, -0.0000, 0.2000, -0.0000,\n",
       "        0.2000, -0.0000, 0.5000, -0.0000, 0.1773, 0.0833, -0.0000, 0.2565, 0.5000,\n",
       "        0.2500, 0.0667, 0.2000, 0.1667, 0.0909, -0.0000, -0.0000, 0.0833])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = metrics.MultilabelAUPRC(num_labels=len(emotions), average=None)\n",
    "metric.update(out, data[2])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f0d7eed-24ae-4ba2-82bc-b561468d4f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 26])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "955bd871-3dd7-42b3-9561-4f0419cee9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 26])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20c70605-b1d8-412f-8d1d-81d82b217a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f693c2d3-6973-4dc3-a93a-815bdf359d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape[0] / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fea0891-ae49-4314-a97d-db3ca1674bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 200, 200])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec8ead55-284e-4eb2-91d3-156db0c7edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 26])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "144f6b92-8925-4790-a150-1fef40303f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a83741-bdd7-424b-bcb4-dba63150cc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5eb78ae-fa57-4cfa-8750-1d31febfc3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5698, 0.7862, 0.7664, 0.6679, 0.5959, 0.7587, 0.7810, 0.5231, 0.5032,\n",
       "         0.6645, 0.7654, 0.5479, 0.6572, 0.7612, 0.7904, 0.7040, 0.7348, 0.6104,\n",
       "         0.5603, 0.6832, 0.6952, 0.9693, 0.5510, 0.5029, 0.7823, 0.6094],\n",
       "        [0.5439, 0.5876, 0.2667, 1.1274, 0.3508, 0.7186, 1.2768, 1.1940, 0.8091,\n",
       "         0.8100, 0.5317, 0.5784, 0.4584, 0.8248, 0.5248, 0.4268, 0.8264, 0.4869,\n",
       "         0.3836, 1.1287, 0.8039, 0.8637, 0.5582, 0.3649, 0.9937, 0.5323],\n",
       "        [0.5015, 0.4972, 0.6327, 0.6324, 0.5582, 0.7617, 0.6780, 0.7400, 0.4364,\n",
       "         0.7839, 0.6695, 0.9016, 0.5458, 0.8100, 0.7402, 0.6117, 0.6556, 0.7592,\n",
       "         0.4259, 0.6794, 0.7276, 0.7482, 0.5616, 0.4885, 1.0681, 0.6824],\n",
       "        [0.6377, 0.4700, 0.4240, 0.6734, 0.8766, 0.8690, 0.7802, 0.6464, 0.5150,\n",
       "         0.4866, 0.6999, 0.5250, 0.6585, 0.7404, 0.8748, 0.6310, 0.5452, 0.5402,\n",
       "         0.6032, 0.7431, 0.7918, 0.9087, 0.7571, 0.4751, 0.8918, 0.8567],\n",
       "        [0.4619, 0.4957, 0.6861, 0.7400, 0.6055, 0.6578, 0.8895, 0.8161, 0.4428,\n",
       "         0.8060, 0.6497, 0.6061, 0.6045, 0.8630, 0.6732, 0.5058, 0.7686, 0.8174,\n",
       "         0.4803, 0.7612, 0.8451, 0.9152, 0.7482, 0.5367, 0.7782, 0.6217],\n",
       "        [0.6136, 0.6335, 0.5584, 0.7305, 0.5484, 0.7986, 0.9737, 0.6004, 0.4801,\n",
       "         0.8198, 0.7613, 0.6315, 0.8847, 0.5582, 0.9845, 0.6045, 0.6432, 0.6245,\n",
       "         0.9040, 1.0086, 0.4831, 0.8532, 0.5858, 0.6800, 1.0222, 0.6841],\n",
       "        [0.3001, 0.5130, 0.4806, 0.7101, 0.2829, 1.5689, 0.7586, 0.7102, 0.3238,\n",
       "         0.4967, 1.0340, 0.6655, 0.5602, 0.4970, 1.7538, 0.1765, 1.0409, 0.4414,\n",
       "         0.4418, 0.7045, 1.0798, 1.0842, 0.5805, 0.1792, 0.8472, 0.3433],\n",
       "        [0.5432, 0.6331, 0.5322, 0.8405, 0.5413, 0.9412, 1.0946, 0.8096, 0.6469,\n",
       "         0.7691, 0.9862, 0.7016, 0.6783, 0.8250, 0.8813, 0.5030, 0.6048, 0.5187,\n",
       "         0.8344, 0.6710, 0.7958, 0.5473, 0.6167, 0.5760, 0.8468, 0.5807],\n",
       "        [0.5335, 0.5013, 0.6264, 0.8875, 0.6269, 0.6244, 0.6859, 0.8445, 0.5522,\n",
       "         0.7061, 0.6308, 0.9802, 0.7888, 0.7473, 0.5460, 0.5365, 0.7157, 0.6359,\n",
       "         0.5859, 0.5543, 0.6293, 0.6702, 0.9478, 0.7058, 0.7108, 0.4511],\n",
       "        [0.5655, 0.6144, 0.5213, 0.5292, 0.4668, 0.9351, 0.9280, 0.7090, 0.4774,\n",
       "         0.5379, 0.7996, 0.4575, 0.6054, 0.8541, 0.7407, 0.6639, 0.5693, 0.5490,\n",
       "         0.3944, 0.5656, 0.5624, 0.7310, 0.6874, 0.6509, 0.8666, 0.6389],\n",
       "        [0.5100, 0.8450, 0.6776, 0.7400, 0.4239, 0.5657, 1.1419, 0.8727, 0.5456,\n",
       "         0.6475, 0.7988, 0.7146, 0.5835, 0.5456, 0.7787, 0.4972, 0.8244, 0.7236,\n",
       "         0.3692, 0.5618, 0.8200, 0.4242, 0.7663, 0.4456, 1.4376, 0.6559],\n",
       "        [0.7790, 0.6760, 0.5971, 0.5584, 0.6042, 1.0170, 0.7874, 0.6555, 0.4622,\n",
       "         1.2283, 0.6535, 0.9818, 0.4329, 0.5144, 0.6991, 0.6438, 0.8555, 0.5637,\n",
       "         0.3971, 0.7105, 0.8742, 0.7279, 0.7112, 0.5006, 1.3608, 0.5100],\n",
       "        [0.3160, 0.3972, 0.4559, 1.1604, 0.9484, 0.6870, 0.9222, 0.9524, 0.4614,\n",
       "         0.8852, 0.7509, 0.8882, 0.7184, 0.7220, 0.9691, 0.7235, 0.7936, 0.4344,\n",
       "         0.3992, 0.5614, 0.5648, 0.9380, 0.6471, 0.5170, 0.7628, 0.5624],\n",
       "        [0.6859, 0.5408, 0.4845, 0.5855, 0.7448, 0.5873, 0.8355, 0.6424, 0.5336,\n",
       "         0.7677, 0.7254, 0.5130, 0.9240, 0.8396, 0.7771, 0.7871, 0.6462, 0.6000,\n",
       "         0.4638, 0.8267, 0.5155, 0.9084, 0.9816, 0.4536, 0.7317, 0.7226],\n",
       "        [0.4682, 0.5846, 0.5097, 0.6473, 0.7453, 0.6310, 0.8013, 0.6275, 0.4572,\n",
       "         0.6582, 0.7359, 0.7332, 0.4979, 0.7644, 1.0065, 0.5109, 0.6433, 0.6390,\n",
       "         0.4642, 1.0372, 0.5262, 0.8348, 0.8352, 0.6282, 0.8881, 0.7780],\n",
       "        [0.5199, 0.6258, 0.6290, 0.7309, 0.5268, 0.7454, 0.7320, 0.5775, 0.5209,\n",
       "         0.6284, 0.5876, 0.5815, 0.5610, 0.8750, 0.8511, 0.6614, 0.7355, 0.6391,\n",
       "         0.4800, 0.6774, 0.5811, 0.8932, 0.7141, 0.5926, 1.1270, 0.7248]],\n",
       "       grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(out, data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c91576bc-2ec0-423b-9dd3-1930e06ab550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Affection', 'Fatigue', 'Happiness', 'Sensitivity'], dtype='<U15')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(emotions)[out[0] > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7f46ce3-818b-4280-ba34-dced46f1fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning'],\n",
       "       ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
       "        'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
       "        'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem',\n",
       "        'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
       "        'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise',\n",
       "        'Sympathy', 'Yearning']], dtype='<U15')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(emotions, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2389e9a6-b259-4fda-a500-0207ad5934da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype='<U15')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 9\n",
    "thres = 0.7\n",
    "\n",
    "emotions[out[idx] > thres]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
