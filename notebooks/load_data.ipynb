{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe59066-6da8-4a15-9a3a-e6adc09272a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, write_jpeg, decode_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fe0ff8-0dd0-4439-8ece-02d0e46404b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
    "            'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
    "            'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
    "\n",
    "emotion_encode = {e: i for i, e in enumerate(emotions)}\n",
    "emotion_decode = {i: e for e, i in emotion_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c81ac5-5a04-4a5b-9961-aa05471dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoticDataset(Dataset):\n",
    "    def __init__(self, subject_size, context_size, anns_dir, img_dir):\n",
    "        anns = scio.loadmat(anns_dir)[\"train\"]\n",
    "        self.anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)\n",
    "        self.img_dir = img_dir\n",
    "        self.subject_transform = transforms.Resize(subject_size)\n",
    "        self.context_transform = transforms.Resize(context_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.anns.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.anns[idx]\n",
    "\n",
    "        img_loc = \"../data/cvpr_emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item()\n",
    "        context_img = read_image(img_loc)\n",
    "        \n",
    "        bbox =  ann[\"person\"][\"body_bbox\"][0][0][0].astype(int)\n",
    "        subject_img = crop(context_img, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "        label = np.zeros(len(emotions))\n",
    "        ems = [i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0].item()[0][0]]\n",
    "        for e in ems:\n",
    "            label[emotion_encode[e]] = 1.\n",
    "\n",
    "        subject_img = self.subject_transform(subject_img.float())\n",
    "        context_img = self.context_transform(context_img.float())\n",
    "\n",
    "        return subject_img, context_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f292d8-7432-40ab-a49a-88f19bb998fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmoticDataset(\"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\")\n",
    "train_dataloader = DataLoader(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f56444d-e2ab-4c99-baf1-b5e91505b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = scio.loadmat('../data/pami/Annotations/Annotations.mat')\n",
    "anns = anns[\"train\"][0]\n",
    "# anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a85aa7d-1a42-441a-9359-7190ea6761c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17077,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65310b8-8911-4bd0-9437-05f35add203d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ade20k/images', 'emodb_small/images', 'framesdb/images', 'mscoco/images'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(map(lambda x: x[\"folder\"].item(), iter(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e24bf98-5504-4efc-9240-d7453d618c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = sorted(list(set(j.item() for ann in anns for i in ann[\"person\"][\"annotations_categories\"][0] for j in i.item()[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0610a9a6-fbbc-40b8-8e8e-5b46c7be4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6272 11617]\n",
      "[112 150]\n",
      "[558.01680623 738.75464074]\n"
     ]
    }
   ],
   "source": [
    "sizes = np.array(list(map(lambda x: [x[\"image_size\"][\"n_row\"].item().item(), x[\"image_size\"][\"n_col\"].item().item()], iter(anns))))\n",
    "print(np.max(sizes, axis=0))\n",
    "print(np.min(sizes, axis=0))\n",
    "print(np.average(sizes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2138682d-012a-46fd-ad34-f2651d4e9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4337 4426]\n",
      "[22 34]\n",
      "[232.18199918 347.39585407]\n"
     ]
    }
   ],
   "source": [
    "boxes = [ann[\"person\"][\"body_bbox\"][0][0][0] for ann in anns]\n",
    "box_sizes = np.array([[b[2] - b[0], b[3] - b[1]] for b in boxes], dtype=np.uint16)\n",
    "print(np.max(box_sizes, axis=0))\n",
    "print(np.min(box_sizes, axis=0))\n",
    "print(np.average(box_sizes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5745f1a2-193a-4fa0-8294-0304903ff089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "683c56d0-2ab7-4abb-be51-52346878597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22aab3a0-14cb-40d6-889b-17adc008219c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_idx = list()\n",
    "for i, ann in enumerate(anns):\n",
    "    for s in range(len(ann[\"person\"][0])):\n",
    "        data_idx.append([i, s])\n",
    "\n",
    "len(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d54130bc-8219-457a-b4de-cf63e94988a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 633\n",
    "idx, p_idx = data_idx[idx]\n",
    "ann = anns[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b3a6e18-6a15-4a72-b77d-41487b53ba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('filename', 'O'), ('folder', 'O'), ('image_size', 'O'), ('original_database', 'O'), ('person', 'O')])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30f788a9-d7d1-43ae-8ff0-0cd80f63cded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscoco/images/COCO_train2014_000000204837.jpg'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"folder\"].item() + '/' + ann[\"filename\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef012570-e3d4-44ea-ae17-8e9a8a392030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = read_image(\"../data/pami/emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item())\n",
    "image = (tf(Image.open(\"../data/pami/emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item())) * 255).to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd610b1e-f881-4028-9230-e7409b262b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 480, 640])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11598e8d-15ab-462a-8a5e-71ad4656a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox =  ann[\"person\"][\"body_bbox\"][0][p_idx][0].astype(int)\n",
    "cropped = crop(image, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "write_jpeg(cropped, \"../data/try_crop/cropped.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15b3cc68-367a-4ec5-a625-9cad8dff2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bbox = draw_bounding_boxes(image, torch.tensor([[bbox[0], bbox[1], bbox[2], bbox[3]]]), colors=[\"red\"], width=4)\n",
    "write_jpeg(draw_bbox, \"../data/try_crop/draw_bbox.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "728d926a-fd11-40ca-b690-784050a0c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[427, 640]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ann[\"image_size\"][\"n_row\"].item().item(), ann[\"image_size\"][\"n_col\"].item().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4481167a-2f02-434f-aa7e-1018b2923b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscoco'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"original_database\"][\"name\"].item().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6f0216e-aac2-42cd-b0ac-2a4c4c18d04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[(array([[94268]], dtype=int32), array([[530216]], dtype=int32))]],\n",
       "              dtype=[('image_id', 'O'), ('annotations_id', 'O')])                 ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"original_database\"][\"info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1018ed9-ed8e-48bc-96ff-46d91811aca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('body_bbox', 'O'), ('annotations_categories', 'O'), ('combined_categories', 'O'), ('annotations_continuous', 'O'), ('combined_continuous', 'O'), ('gender', 'O'), ('age', 'O')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1e6184fe-3eb2-4315-9170-5c971fe3d275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([238, 113, 374, 335], dtype=uint16)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"body_bbox\"].item()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddc1fbb3-dd2d-4006-9a8c-0da680e7f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confidence', 'Engagement']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0][0][0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f596c9e5-7536-4f4b-b28d-fad37d749c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(array([[array(['Happiness'], dtype='<U9')]], dtype=object),)]],\n",
       "      dtype=[('categories', 'O')])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"annotations_categories\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91a71c27-a9db-4960-8335-91865726c777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(array([[array(['Happiness'], dtype='<U9')]], dtype=object),)]],\n",
       "      dtype=[('categories', 'O')])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"annotations_categories\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90d6aaba-22a0-4824-a5f0-7a14ee57725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happiness']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0][0][0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "29ad40d8-76bd-494d-aabe-578adcb80f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[(array([[7]], dtype=uint8), array([[4]], dtype=uint8), array([[6]], dtype=uint8))]],\n",
       "              dtype=[('valence', 'O'), ('arousal', 'O'), ('dominance', 'O')])                       ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"annotations_continuous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "148e5161-bd00-46a4-8db2-5b97e19150cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array(['Male'], dtype='<U4')]], dtype=object)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ffe4f159-6356-4da0-a084-5f733d926e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array(['Adult'], dtype='<U5')]], dtype=object)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"age\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
