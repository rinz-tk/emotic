{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe59066-6da8-4a15-9a3a-e6adc09272a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fe0ff8-0dd0-4439-8ece-02d0e46404b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
    "            'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
    "            'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
    "\n",
    "emotion_encode = {e: i for i, e in enumerate(emotions)}\n",
    "emotion_decode = {i: e for e, i in emotion_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c81ac5-5a04-4a5b-9961-aa05471dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoticDataset(Dataset):\n",
    "    def __init__(self, subject_size, context_size, anns_dir, img_dir):\n",
    "        anns = scio.loadmat(anns_dir)[\"train\"]\n",
    "        self.anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)\n",
    "        self.img_dir = img_dir\n",
    "        self.subject_transform = transforms.Resize(subject_size)\n",
    "        self.context_transform = transforms.Resize(context_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.anns.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.anns[idx]\n",
    "\n",
    "        img_loc = \"../data/cvpr_emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item()\n",
    "        context_img = read_image(img_loc)\n",
    "        \n",
    "        bbox =  ann[\"person\"][\"body_bbox\"][0][0][0].astype(int)\n",
    "        subject_img = crop(context_img, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "        label = np.zeros(len(emotions))\n",
    "        ems = [i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0].item()[0][0]]\n",
    "        for e in ems:\n",
    "            label[emotion_encode[e]] = 1.\n",
    "\n",
    "        subject_img = self.subject_transform(subject_img.float())\n",
    "        context_img = self.context_transform(context_img.float())\n",
    "\n",
    "        return subject_img, context_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f292d8-7432-40ab-a49a-88f19bb998fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EmoticDataset.__init__() missing 2 required positional arguments: 'anns_dir' and 'img_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mEmoticDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/Annotations/Annotations.mat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/cvpr_emotic/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_data)\n",
      "\u001b[0;31mTypeError\u001b[0m: EmoticDataset.__init__() missing 2 required positional arguments: 'anns_dir' and 'img_dir'"
     ]
    }
   ],
   "source": [
    "train_data = EmoticDataset(\"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\")\n",
    "train_dataloader = DataLoader(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396beb2b-b25a-4f39-9aef-b684900927d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'test', 'train', 'val'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f56444d-e2ab-4c99-baf1-b5e91505b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = scio.loadmat('../data/Annotations/Annotations.mat')\n",
    "anns = anns[\"train\"]\n",
    "anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a85aa7d-1a42-441a-9359-7190ea6761c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1705,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a65310b8-8911-4bd0-9437-05f35add203d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ade20k/images', 'emodb_small/images', 'mscoco/images'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(map(lambda x: x[\"folder\"].item(), iter(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5e24bf98-5504-4efc-9240-d7453d618c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = sorted(list(set(j.item() for ann in anns for i in ann[\"person\"][\"annotations_categories\"][0] for j in i.item()[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0610a9a6-fbbc-40b8-8e8e-5b46c7be4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5616 7360]\n",
      "[112 150]\n",
      "[472.44436702 622.68873403]\n"
     ]
    }
   ],
   "source": [
    "sizes = np.array(list(map(lambda x: [x[\"image_size\"][\"n_row\"].item().item(), x[\"image_size\"][\"n_col\"].item().item()], iter(anns))))\n",
    "print(np.max(sizes, axis=0))\n",
    "print(np.min(sizes, axis=0))\n",
    "print(np.average(sizes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2138682d-012a-46fd-ad34-f2651d4e9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3697 4138]\n",
      "[27 44]\n",
      "[212.63902439 307.7416957 ]\n"
     ]
    }
   ],
   "source": [
    "boxes = [ann[\"person\"][\"body_bbox\"][0][0][0] for ann in anns]\n",
    "box_sizes = np.array([[b[2] - b[0], b[3] - b[1]] for b in boxes], dtype=np.uint16)\n",
    "print(np.max(box_sizes, axis=0))\n",
    "print(np.min(box_sizes, axis=0))\n",
    "print(np.average(box_sizes, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5745f1a2-193a-4fa0-8294-0304903ff089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1705"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d54130bc-8219-457a-b4de-cf63e94988a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 811\n",
    "ann = anns[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b3a6e18-6a15-4a72-b77d-41487b53ba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('filename', 'O'), ('folder', 'O'), ('image_size', 'O'), ('original_database', 'O'), ('person', 'O')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30f788a9-d7d1-43ae-8ff0-0cd80f63cded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscoco/images/COCO_train2014_000000008468.jpg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"folder\"].item() + '/' + ann[\"filename\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "55cfa3a3-75eb-46af-90da-068413b6269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image(\"../data/cvpr_emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a9bfbe31-4611-4fce-b090-053556d28d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 438, 640])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "11598e8d-15ab-462a-8a5e-71ad4656a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox =  ann[\"person\"][\"body_bbox\"][0][0][0].astype(int)\n",
    "cropped = crop(image, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "write_jpeg(cropped, \"../data/try_crop/cropped.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "728d926a-fd11-40ca-b690-784050a0c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[427, 640]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ann[\"image_size\"][\"n_row\"].item().item(), ann[\"image_size\"][\"n_col\"].item().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4481167a-2f02-434f-aa7e-1018b2923b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscoco'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"original_database\"][\"name\"].item().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a6f0216e-aac2-42cd-b0ac-2a4c4c18d04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[(array([[481931]], dtype=int32), array([[473666]], dtype=int32))]],\n",
       "              dtype=[('image_id', 'O'), ('annotations_id', 'O')])                  ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"original_database\"][\"info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1018ed9-ed8e-48bc-96ff-46d91811aca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('body_bbox', 'O'), ('annotations_categories', 'O'), ('combined_categories', 'O'), ('annotations_continuous', 'O'), ('combined_continuous', 'O'), ('gender', 'O'), ('age', 'O')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1e6184fe-3eb2-4315-9170-5c971fe3d275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([238, 113, 374, 335], dtype=uint16)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"body_bbox\"].item()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddc1fbb3-dd2d-4006-9a8c-0da680e7f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confidence', 'Engagement']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0][0][0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f596c9e5-7536-4f4b-b28d-fad37d749c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(array([[array(['Happiness'], dtype='<U9')]], dtype=object),)]],\n",
       "      dtype=[('categories', 'O')])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"annotations_categories\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91a71c27-a9db-4960-8335-91865726c777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(array([[array(['Happiness'], dtype='<U9')]], dtype=object),)]],\n",
       "      dtype=[('categories', 'O')])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"annotations_categories\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90d6aaba-22a0-4824-a5f0-7a14ee57725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happiness']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0][0][0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "29ad40d8-76bd-494d-aabe-578adcb80f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[(array([[7]], dtype=uint8), array([[4]], dtype=uint8), array([[6]], dtype=uint8))]],\n",
       "              dtype=[('valence', 'O'), ('arousal', 'O'), ('dominance', 'O')])                       ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"annotations_continuous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "148e5161-bd00-46a4-8db2-5b97e19150cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array(['Male'], dtype='<U4')]], dtype=object)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ffe4f159-6356-4da0-a084-5f733d926e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array(['Adult'], dtype='<U5')]], dtype=object)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann[\"person\"][\"age\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
