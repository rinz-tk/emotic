{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e12f5311-9ea5-451e-a01c-3c22ddc8bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a44dcf1-3798-4c13-b0a9-f16735166b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
    "            'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace',\n",
    "            'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
    "\n",
    "emotion_encode = {e: i for i, e in enumerate(emotions)}\n",
    "emotion_decode = {i: e for e, i in emotion_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02c81ac5-5a04-4a5b-9961-aa05471dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoticDataset(Dataset):\n",
    "    def __init__(self, subject_size, context_size, anns_dir, img_dir):\n",
    "        anns = scio.loadmat(anns_dir)[\"train\"]\n",
    "        self.anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)\n",
    "        self.img_dir = img_dir\n",
    "        self.subject_transform = transforms.Resize(subject_size)\n",
    "        self.context_transform = transforms.Resize(context_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.anns.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.anns[idx]\n",
    "\n",
    "        img_loc = \"../data/cvpr_emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item()\n",
    "        context_img = read_image(img_loc)\n",
    "        \n",
    "        bbox =  ann[\"person\"][\"body_bbox\"][0][0][0].astype(int)\n",
    "        subject_img = crop(context_img, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "        label = np.zeros(len(emotions))\n",
    "        ems = [i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0].item()[0][0]]\n",
    "        for e in ems:\n",
    "            label[emotion_encode[e]] = 1.\n",
    "\n",
    "        subject_img = self.subject_transform(subject_img.float())\n",
    "        context_img = self.context_transform(context_img.float())\n",
    "\n",
    "        return subject_img, context_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af59d47-7c33-4ef2-8ef7-05765e414d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_size = (50, 50)\n",
    "context_size = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5088280b-72bd-41d0-b80b-f3a6f1fa77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmoticDataset(subject_size, context_size, \"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\")\n",
    "train_dataloader = DataLoader(train_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be692383-5210-4ae2-b34c-26c2a2ad1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_branch():\n",
    "    return nn.Sequential(\n",
    "               nn.Conv2d(3, 96, (11, 1), stride=(4, 1)),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(96),\n",
    "               nn.Conv2d(96, 96, (1, 11), stride=(1, 4)),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(96),\n",
    "               nn.MaxPool2d(3, stride=2),\n",
    "               nn.Conv2d(96, 256, (1, 5), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(256),\n",
    "               nn.Conv2d(256, 256, (5, 1), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(256),\n",
    "               nn.MaxPool2d(3, stride=2),\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "               nn.Conv2d(256, 384, (1, 3), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(384),\n",
    "               nn.Conv2d(384, 384, (3, 1), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(384),\n",
    "               \n",
    "               nn.Conv2d(384, 384, (1, 3), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(384),\n",
    "               nn.Conv2d(384, 384, (3, 1), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(384),\n",
    "    \n",
    "               nn.Conv2d(384, 256, (1, 3), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(256),\n",
    "               nn.Conv2d(256, 256, (3, 1), padding=\"same\"),\n",
    "               nn.ReLU(),\n",
    "               nn.BatchNorm2d(256),\n",
    "               nn.MaxPool2d(3, stride=2)\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "002904bb-cc8f-4ea4-8c81-7959c3a0ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.subject = net_branch()\n",
    "        self.context = net_branch()\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "        )\n",
    "\n",
    "    def forward(self, s, c):\n",
    "        s = self.subject(s)\n",
    "        s = torch.flatten(s, start_dim=1)\n",
    "        \n",
    "        c = self.context(c)\n",
    "        c = torch.flatten(c, start_dim=1)\n",
    "\n",
    "        x = torch.cat((s, c), dim=1)\n",
    "        x = self.fusion(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b69484bb-4517-4aa8-97fd-d17061318a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "32373e38-3afe-42ee-a2a0-d2aecd7ac194",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(iter(train_dataloader))\n",
    "save = n(first[0], first[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0fea0891-ae49-4314-a97d-db3ca1674bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 50, 50])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ec8ead55-284e-4eb2-91d3-156db0c7edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 31232])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
