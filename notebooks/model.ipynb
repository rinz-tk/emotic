{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12f5311-9ea5-451e-a01c-3c22ddc8bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from torcheval import metrics\n",
    "\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a44dcf1-3798-4c13-b0a9-f16735166b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = np.array(['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection',\n",
    "                     'Disquietment', 'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear',\n",
    "                     'Happiness', 'Pain', 'Peace', 'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning'])\n",
    "\n",
    "emotion_encode = {e: i for i, e in enumerate(emotions)}\n",
    "emotion_decode = {i: e for e, i in emotion_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c81ac5-5a04-4a5b-9961-aa05471dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoticDataset(Dataset):\n",
    "    def __init__(self, data_type, subject_size, context_size, anns_dir, img_dir, alex=False):\n",
    "        anns = scio.loadmat(anns_dir)[data_type]\n",
    "        self.anns = np.fromiter(filter(lambda x: x[\"folder\"].item() != \"framesdb/images\", iter(anns[0])), dtype=anns.dtype)\n",
    "        self.img_dir = img_dir\n",
    "        self.subject_transform = transforms.Resize(subject_size)\n",
    "\n",
    "        if alex:\n",
    "            self.context_transform = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            self.context_transform = transforms.Resize(context_size)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.anns.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.anns[idx]\n",
    "\n",
    "        img_loc = \"../data/cvpr_emotic/\" + ann[\"folder\"].item() + '/' + ann[\"filename\"].item()\n",
    "        context_img = read_image(img_loc)\n",
    "        \n",
    "        bbox =  ann[\"person\"][\"body_bbox\"][0][0][0].astype(int)\n",
    "        subject_img = crop(context_img, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "        label = np.zeros(len(emotions), dtype=np.float32)\n",
    "        ems = [i.item() for i in ann[\"person\"][\"annotations_categories\"][0][0][0][0][0][0]]\n",
    "        for e in ems:\n",
    "            label[emotion_encode[e]] = 1.\n",
    "            \n",
    "        if subject_img.shape[0] == 1:\n",
    "            subject_img = subject_img.repeat(3, 1, 1)\n",
    "            \n",
    "        if context_img.shape[0] == 1:\n",
    "            context_img = context_img.repeat(3, 1, 1)\n",
    "\n",
    "        subject_img = self.subject_transform(subject_img.float())\n",
    "        context_img = self.context_transform(context_img.float())\n",
    "\n",
    "        return subject_img, context_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be692383-5210-4ae2-b34c-26c2a2ad1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_branch():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 96, (11, 1), stride=(4, 1)),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(96),\n",
    "        nn.Conv2d(96, 96, (1, 11), stride=(1, 4)),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(96),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        \n",
    "        nn.Conv2d(96, 256, (1, 5), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(256, 256, (5, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2),\n",
    "        \n",
    "        nn.Conv2d(256, 384, (1, 3), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        nn.Conv2d(384, 384, (3, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        \n",
    "        nn.Conv2d(384, 384, (1, 3), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        nn.Conv2d(384, 384, (3, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(384),\n",
    "        \n",
    "        nn.Conv2d(384, 256, (1, 3), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.Conv2d(256, 256, (3, 1), padding=\"same\"),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.MaxPool2d(3, stride=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "002904bb-cc8f-4ea4-8c81-7959c3a0ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.subject = net_branch()\n",
    "        self.context = net_branch()\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(12800, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, len(emotions)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, s, c):\n",
    "        s = self.subject(s)\n",
    "        s = torch.flatten(s, start_dim=1)\n",
    "        \n",
    "        c = self.context(c)\n",
    "        c = torch.flatten(c, start_dim=1)\n",
    "\n",
    "        x = torch.cat((s, c), dim=1)\n",
    "        x = self.fusion(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2994431-d33f-43ac-b1b1-5d90f2f6fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.subject = net_branch()\n",
    "\n",
    "        alexnet_file = \"../models/alexnet_places365.pth.tar\"\n",
    "        checkpoint = torch.load(alexnet_file, map_location=lambda storage, loc: storage, weights_only=False)\n",
    "        state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "\n",
    "        self.context = models.__dict__[\"alexnet\"](num_classes=365)\n",
    "        self.context.load_state_dict(state_dict)\n",
    "\n",
    "        for param in self.context.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(15616, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, len(emotions)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, s, c):\n",
    "        s = self.subject(s)\n",
    "        s = torch.flatten(s, start_dim=1)\n",
    "        \n",
    "        c = self.context.features(c)\n",
    "        c = torch.flatten(c, start_dim=1)\n",
    "\n",
    "        x = torch.cat((s, c), dim=1)\n",
    "        x = self.fusion(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaef8c15-7fd1-4b37-b710-a22a90b6a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af59d47-7c33-4ef2-8ef7-05765e414d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_size = (224, 224)\n",
    "context_size = (224, 224)\n",
    "\n",
    "train_batch_size = 52\n",
    "val_batch_size = 52\n",
    "test_batch_size = 52\n",
    "num_epochs = 500\n",
    "learning_rate = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c67a657b-c43b-4e9f-9f1c-ccf5200d21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_data = False\n",
    "\n",
    "train_data = EmoticDataset(\"train\", subject_size, context_size, \"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\", alex=alex_data)\n",
    "train_dataloader = DataLoader(train_data, batch_size=train_batch_size)\n",
    "\n",
    "val_data = EmoticDataset(\"val\", subject_size, context_size, \"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\", alex=alex_data)\n",
    "val_dataloader = DataLoader(val_data, batch_size=val_batch_size)\n",
    "\n",
    "test_data = EmoticDataset(\"test\", subject_size, context_size, \"../data/Annotations/Annotations.mat\", \"../data/cvpr_emotic/\", alex=alex_data)\n",
    "test_dataloader = DataLoader(test_data, batch_size=test_batch_size)\n",
    "\n",
    "train_size = len(train_dataloader.dataset)\n",
    "val_size = len(val_dataloader.dataset)\n",
    "test_size = len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31673620-4375-49f3-b666-7a1a4e1642f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "metric = metrics.MultilabelAUPRC(num_labels=len(emotions), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b69484bb-4517-4aa8-97fd-d17061318a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotic_net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "112b19f9-c345-4012-b561-631af4386e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotic_net = PretrainedNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1153d84c-f523-41cd-a30a-c1397e5c7334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(8920666)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.prod(i.data.shape) for i in emotic_net.parameters() if i.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d9e857c2-0e54-41f6-91e7-252a60b960c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(emotic_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34afa7-edb4-4ec7-a532-1cfd0a7020b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    emotic_net.train()\n",
    "    for batch, (S, C, y) in enumerate(train_dataloader):\n",
    "        S, C, y = S.to(device), C.to(device), y.to(device)\n",
    "        pred = emotic_net(S, C)\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss, current = loss.item(), min((batch + 1) * train_batch_size, train_size)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\", end=\"\\r\")\n",
    "        \n",
    "    print()\n",
    "\n",
    "    torch.save(emotic_net.state_dict(), \"../models/basic.pth\")\n",
    "    \n",
    "    metric.reset()\n",
    "    emotic_net.eval()\n",
    "    for batch, (S, C, y) in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            S, C, y = S.to(device), C.to(device), y.to(device)\n",
    "            pred = emotic_net(S, C)\n",
    "            metric.update(pred, y)\n",
    "    \n",
    "            current = min((batch + 1) * val_batch_size, val_size)\n",
    "            print(f\"calculating metric for:  [{current:>4d}/{val_size:>4d}]\", end=\"\\r\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"metric: {metric.compute() * 100:>7f} %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d06294a2-923a-4e74-83f4-74a6a6420672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_emotic_net = Net().to(device)\n",
    "load_emotic_net.load_state_dict(torch.load(\"../models/basic_trained.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93844166-5e76-4ad3-b359-1234348dea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(emotic_net, metric, dataloader, batch_size, size):\n",
    "    metric.reset()\n",
    "    emotic_net.eval()\n",
    "    for batch, (S, C, y) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            S, C, y = S.to(device), C.to(device), y.to(device)\n",
    "            pred = emotic_net(S, C)\n",
    "            metric.update(pred, y)\n",
    "    \n",
    "            current = min((batch + 1) * batch_size, size)\n",
    "            print(f\"calculating metric for:  [{current:>4d}/{size:>4d}]\", end=\"\\r\")\n",
    "    \n",
    "    print()\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2009df76-29a7-4d91-a7ab-cc2cc0c8b804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating metric for:  [3682/3682]\n",
      "metric: 13.611191749572754 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mets = compute_metrics(load_emotic_net, metric, test_dataloader, test_batch_size, test_size)\n",
    "print(f\"metric: {mets * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "28ea4b2d-77f2-4839-a58b-4c392572601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (subject): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 1), stride=(4, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(96, 96, kernel_size=(1, 11), stride=(1, 4))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(96, 256, kernel_size=(1, 5), stride=(1, 1), padding=same)\n",
       "    (8): ReLU()\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 384, kernel_size=(1, 3), stride=(1, 1), padding=same)\n",
       "    (15): ReLU()\n",
       "    (16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (18): ReLU()\n",
       "    (19): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=same)\n",
       "    (21): ReLU()\n",
       "    (22): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Conv2d(384, 256, kernel_size=(1, 3), stride=(1, 1), padding=same)\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (30): ReLU()\n",
       "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (context): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 1), stride=(4, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(96, 96, kernel_size=(1, 11), stride=(1, 4))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(96, 256, kernel_size=(1, 5), stride=(1, 1), padding=same)\n",
       "    (8): ReLU()\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 384, kernel_size=(1, 3), stride=(1, 1), padding=same)\n",
       "    (15): ReLU()\n",
       "    (16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (18): ReLU()\n",
       "    (19): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=same)\n",
       "    (21): ReLU()\n",
       "    (22): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (24): ReLU()\n",
       "    (25): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Conv2d(384, 256, kernel_size=(1, 3), stride=(1, 1), padding=same)\n",
       "    (27): ReLU()\n",
       "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (30): ReLU()\n",
       "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fusion): Sequential(\n",
       "    (0): Linear(in_features=12800, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=26, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotic_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5e7d3d30-2411-4f4a-8b9c-d734fc79192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating metric for:  [3682/3682]\n",
      "Affection       : 10.691 %\n",
      "Anger           : 15.148 %\n",
      "Annoyance       : 10.015 %\n",
      "Anticipation    : 37.276 %\n",
      "Aversion        :  3.424 %\n",
      "Confidence      : 40.458 %\n",
      "Disapproval     :  6.154 %\n",
      "Disconnection   :  6.613 %\n",
      "Disquietment    :  5.648 %\n",
      "Doubt/Confusion :  6.024 %\n",
      "Embarrassment   :  1.495 %\n",
      "Engagement      : 56.543 %\n",
      "Esteem          :  5.682 %\n",
      "Excitement      : 40.755 %\n",
      "Fatigue         :  4.948 %\n",
      "Fear            :  2.251 %\n",
      "Happiness       : 36.746 %\n",
      "Pain            :  3.882 %\n",
      "Peace           : 11.179 %\n",
      "Pleasure        : 17.286 %\n",
      "Sadness         :  7.964 %\n",
      "Sensitivity     :  3.869 %\n",
      "Suffering       :  7.226 %\n",
      "Surprise        :  3.193 %\n",
      "Sympathy        :  6.441 %\n",
      "Yearning        :  2.981 %\n"
     ]
    }
   ],
   "source": [
    "class_metrics = compute_metrics(load_emotic_net, metrics.MultilabelAUPRC(num_labels=len(emotions), device=device, average=None), test_dataloader, test_batch_size, test_size)\n",
    "\n",
    "for e, m in zip(emotions, class_metrics):\n",
    "    print(f\"{e:16}: {m*100:>6.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4103829f-8495-4172-af55-9d917138acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engagement      : 56.543 %\n",
      "Excitement      : 40.755 %\n",
      "Confidence      : 40.458 %\n",
      "Anticipation    : 37.276 %\n",
      "Happiness       : 36.746 %\n",
      "Pleasure        : 17.286 %\n",
      "Anger           : 15.148 %\n",
      "Peace           : 11.179 %\n",
      "Affection       : 10.691 %\n",
      "Annoyance       : 10.015 %\n",
      "Sadness         :  7.964 %\n",
      "Suffering       :  7.226 %\n",
      "Disconnection   :  6.613 %\n",
      "Sympathy        :  6.441 %\n",
      "Disapproval     :  6.154 %\n",
      "Doubt/Confusion :  6.024 %\n",
      "Esteem          :  5.682 %\n",
      "Disquietment    :  5.648 %\n",
      "Fatigue         :  4.948 %\n",
      "Pain            :  3.882 %\n",
      "Sensitivity     :  3.869 %\n",
      "Aversion        :  3.424 %\n",
      "Surprise        :  3.193 %\n",
      "Yearning        :  2.981 %\n",
      "Fear            :  2.251 %\n",
      "Embarrassment   :  1.495 %\n"
     ]
    }
   ],
   "source": [
    "for e, m in sorted(zip(emotions, class_metrics), key=lambda x: -x[1]):\n",
    "    print(f\"{e:16}: {m*100:>6.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20b011ff-2e56-4da8-ac1d-6f71b806fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1325\n",
    "data = train_data[idx]\n",
    "data = (data[0].unsqueeze(0), data[1].unsqueeze(0), torch.tensor(data[2]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32373e38-3afe-42ee-a2a0-d2aecd7ac194",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = load_emotic_net(data[0].to(device), data[1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86e5a646-c083-444c-a033-4094f57e862d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ae01506b-f05c-4f9d-b2dc-65012746e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "62e459e2-dc2d-424e-b383-0121a50b1ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.7358, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out, data[2].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c91576bc-2ec0-423b-9dd3-1930e06ab550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anticipation', 'Aversion', 'Disapproval', 'Disconnection',\n",
       "       'Doubt/Confusion', 'Embarrassment', 'Esteem', 'Fatigue', 'Fear',\n",
       "       'Happiness', 'Suffering'], dtype='<U15')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[out.cpu()[0] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "88605795-f3ae-4379-a9f4-4ab61ef61a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Excitement', 'Pleasure'], dtype='<U15')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[data[2][0] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b6be9-ea60-4881-90b2-5fd9187d552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed4917-54fb-4a33-b2fa-d8859aa41798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23fd04-88ad-48b3-ac64-5640f41297f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
